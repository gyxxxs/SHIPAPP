import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import font_manager
import time
import os
import tempfile
from datetime import datetime
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
import json

# æ–°å¢å¯¼å…¥ - RAGå’Œæ¨¡å‹è¯Šæ–­
try:
    from knowledge_base import KnowledgeBase, init_knowledge_base
    from model_diagnostics import ModelDiagnostics
    RAG_AVAILABLE = True
    MODEL_DIAGNOSTICS_AVAILABLE = True
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    RAG_AVAILABLE = False
    MODEL_DIAGNOSTICS_AVAILABLE = False

# --- matplotlib ä¸­æ–‡å­—ä½“é…ç½® ---
plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'WenQuanYi Micro Hei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# --- 0. ç¯å¢ƒå’Œå·¥å…·å®šä¹‰ (ä¿æŒä¸å˜) ---

class ReportInput(BaseModel):
    """ç”¨äºç”Ÿæˆè¯¦ç»†æ•…éšœè¯Šæ–­æŠ¥å‘Šçš„å·¥å…·"""
    fault_id: str = Field(description="å½“å‰æ•…éšœäº‹ä»¶çš„å”¯ä¸€æ ‡è¯†ID,ä¾‹å¦‚:'EVENT-20251028-001'")
    severity: str = Field(description="æ•…éšœçš„ä¸¥é‡ç¨‹åº¦,ä¾‹å¦‚:'ä¸€çº§é¢„è­¦'æˆ–'äºŒçº§é¢„è­¦'")
    fault_type: str = Field(description="æ•…éšœç±»å‹,å¦‚:'ä¸²è”ç”µå¼§æ•…éšœ'ã€'ç»ç¼˜è€åŒ–'ç­‰")

class StabilityInput(BaseModel):
    """ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡"""

class MaintenanceInput(BaseModel):
    """æ ¹æ®æ•…éšœç±»å‹ç”Ÿæˆç»´æŠ¤å·¥å•"""
    circuit_id: str = Field(description="å›è·¯ç¼–å·,ä¾‹å¦‚:'03å·èˆ±å›è·¯'")
    fault_severity: str = Field(description="æ•…éšœä¸¥é‡ç¨‹åº¦")
    maintenance_type: str = Field(description="ç»´æŠ¤ç±»å‹:é¢„é˜²æ€§/ç´§æ€¥")

def generate_diagnostic_report(fault_id: str, severity: str, fault_type: str) -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–çš„æ•…éšœè¯Šæ–­æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    report_data = {
        "report_id": f"RPT-{fault_id}",
        "timestamp": timestamp,
        "fault_severity": severity,
        "fault_type": fault_type,
        "dl_confidence": "97.5%",
        "root_cause": "é«˜æŒ¯åŠ¨åŒºåŸŸç”µç¼†å›ºå®šä»¶è€åŒ–æ¾åŠ¨å¯¼è‡´çš„ä¸²è”ç”µå¼§æ•…éšœ",
        "maintenance_advice": "ç«‹å³è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥,ç´§å›ºè¿æ¥ä»¶,å‚è€ƒCCSè§„èŒƒç¬¬5.4.1æ¡",
        "risk_level": "é«˜" if "äºŒçº§" in severity else "ä¸­"
    }
    return f"ã€è¯Šæ–­æŠ¥å‘Šã€‘{json.dumps(report_data, ensure_ascii=False, indent=2)}"

def check_system_stability() -> str:
    """æŸ¥è¯¢ç³»ç»Ÿç¨³å®šæ€§çŠ¶æ€"""
    stability_data = {
        "edge_compute_load": "38%",
        "inference_latency": "15ms",
        "communication_latency": "45ms",
        "model_accuracy": "97.5%",
        "system_status": "ç¨³å®š"
    }
    return f"ã€ç³»ç»ŸçŠ¶æ€ã€‘{json.dumps(stability_data, ensure_ascii=False)}"

def generate_maintenance_order(circuit_id: str, fault_severity: str, maintenance_type: str) -> str:
    """ç”Ÿæˆç»´æŠ¤å·¥å•"""
    order_data = {
        "order_id": f"MO-{datetime.now().strftime('%Y%m%d%H%M')}",
        "circuit": circuit_id,
        "maintenance_type": maintenance_type,
        "priority": "ç´§æ€¥" if "äºŒçº§" in fault_severity else "é«˜",
        "required_tools": "çº¢å¤–çƒ­åƒä»ª,åŠ›çŸ©æ‰³æ‰‹,ç»ç¼˜æµ‹è¯•ä»ª",
        "estimated_duration": "2å°æ—¶",
        "safety_requirements": "æ–­ç”µæ“ä½œ,ç©¿æˆ´PPE"
    }
    return f"ã€ç»´æŠ¤å·¥å•ã€‘{json.dumps(order_data, ensure_ascii=False)}"

AVAILABLE_TOOLS = {
    "generate_diagnostic_report": generate_diagnostic_report,
    "check_system_stability": check_system_stability,
    "generate_maintenance_order": generate_maintenance_order,
}

# --- 1. å¢å¼ºçš„æ•°æ®æ¨¡æ‹Ÿ (ä¿æŒä¸å˜) ---
def simulate_current_data(t, fault_scenario="normal", prediction_mode=False):
    """
    æ¨¡æ‹Ÿæ›´çœŸå®çš„èˆ¹èˆ¶ç”µæµæ•°æ®
    """
    base_frequency = 50
    # æ¨¡æ‹Ÿæ³¢å½¢æ»šåŠ¨ï¼ŒåŠ å…¥ä¸€ä¸ªéšæœºç›¸ä½åç§»
    phase_offset = time.time() * 2 * np.pi * base_frequency / 1000 
    
    time_series = np.linspace(0, 2 / base_frequency, t)  # 2ä¸ªå‘¨æœŸ
    current = 10 * np.sin(2 * np.pi * base_frequency * time_series + phase_offset)
    
    # åŸºç¡€å™ªå£°
    current += np.random.normal(0, 0.05, t)
    
    if fault_scenario == "early_arc":
        # æ—©æœŸç”µå¼§ç‰¹å¾:é—´æ­‡æ€§é«˜é¢‘å™ªå£°
        mask = (time_series % 0.1 < 0.02)
        high_freq = np.sin(2 * np.pi * 5000 * time_series) * 0.3
        current += high_freq * mask
        
    elif fault_scenario == "severe_arc":
        # ä¸¥é‡ç”µå¼§ç‰¹å¾:æŒç»­é«˜é¢‘å™ªå£°+å¹…å€¼å˜åŒ–
        high_freq = np.sin(2 * np.pi * 3000 * time_series) * 0.8
        current += high_freq + 2 * np.random.rand(t)
        
    elif fault_scenario == "motor_start":
        # ç”µæœºå¯åŠ¨å¹²æ‰°
        startup_effect = 3 * np.exp(-time_series * 2) * np.sin(2 * np.pi * 100 * time_series)
        current += startup_effect

    if prediction_mode:
        # é¢„æµ‹æ¨¡å¼ä¸‹çš„è¶‹åŠ¿ç‰¹å¾
        trend_factor = (time.time() - st.session_state.last_update) / 10 
        trend = 0.5 * np.exp(-time_series * 3) * np.sin(2 * np.pi * 150 * time_series) * (1 + trend_factor)
        current += trend

    return time_series * 1000, current

# --- 2. å¢å¼ºçš„æ¨¡å‹æ¨ç† (é›†æˆçœŸå®æ¨¡å‹) ---
@st.cache_resource
def get_model_diagnostics():
    """è·å–æ¨¡å‹è¯Šæ–­å®ä¾‹"""
    if MODEL_DIAGNOSTICS_AVAILABLE:
        return ModelDiagnostics()
    return None

def dl_model_inference(data, fault_scenario):
    """ä½¿ç”¨çœŸå®æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæ¨ç†"""
    model_diagnostics = get_model_diagnostics()
    
    if model_diagnostics is not None:
        # ä½¿ç”¨çœŸå®æ¨¡å‹æ¨ç†
        status_text, confidence, fault_type = model_diagnostics.inference(data, fault_scenario)
        return status_text, confidence, fault_type
    else:
        # å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼
        if fault_scenario == "severe_arc":
            return "äºŒçº§é¢„è­¦ (æ•…éšœç¡®è®¤)", 97.5, "severe_arc"
        elif fault_scenario == "early_arc":
            if 'early_arc_confidence' not in st.session_state:
                 st.session_state.early_arc_confidence = 70.0
            
            st.session_state.early_arc_confidence = min(90.0, st.session_state.early_arc_confidence + 0.5) 

            if st.session_state.early_arc_confidence > 70.0:
                return "ä¸€çº§é¢„è­¦ (é¢„æµ‹é£é™©)", st.session_state.early_arc_confidence, "early_arc"
            else:
                return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 5.0, "normal"
                
        elif fault_scenario == "motor_start":
            return "å¹²æ‰°ä¿¡å· (ç”µæœºå¯åŠ¨)", 10.0, "motor_start"
        else:
            st.session_state.early_arc_confidence = 70.0 if 'early_arc_confidence' in st.session_state else 70.0
            return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 2.0, "normal"

# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘ (é›†æˆRAG) ---
@st.cache_resource
def get_gemini_client():
    """å®‰å…¨åœ°è·å– Gemini å®¢æˆ·ç«¯"""
    try:
        if "gemini_api_key" not in st.secrets:
            return None 
        GEMINI_API_KEY = st.secrets["gemini_api_key"]
        return genai.Client(api_key=GEMINI_API_KEY)
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None

@st.cache_resource
def get_knowledge_base():
    """è·å–çŸ¥è¯†åº“å®ä¾‹"""
    if RAG_AVAILABLE:
        return init_knowledge_base()
    return None

def gemini_agent_response(user_query: str, system_status: dict):
    """å¢å¼ºçš„æ™ºèƒ½ä½“å“åº”å‡½æ•° - é›†æˆRAG"""
    client = get_gemini_client()
    
    if client is None:
        return "âš ï¸ Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼ˆå¯èƒ½ç¼ºå°‘ API Keyï¼‰ï¼Œæ— æ³•æ‰§è¡Œ AI æ¨ç†ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚"
        
    status_context = (
        f"ã€å®æ—¶ç³»ç»ŸçŠ¶æ€ã€‘\n"
        f"- æ£€æµ‹çŠ¶æ€: {system_status['detection_status']}\n"
        f"- ç½®ä¿¡åº¦: {system_status['confidence']:.1f}%\n" 
        f"- æ•…éšœç±»å‹: {system_status['fault_type']}\n"
        f"- å›è·¯ç¼–å·: {system_status['circuit_id']}\n"
        f"- æ—¶é—´æˆ³: {system_status['timestamp']}\n"
    )
    
    # ä½¿ç”¨RAGæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    kb = get_knowledge_base()
    if kb is not None:
        retrieval_results = kb.format_retrieval_results(user_query, k=5)
    else:
        # å›é€€åˆ°ç¡¬ç¼–ç çŸ¥è¯†
        retrieval_results = (
        "ã€RAGæ£€ç´¢ç»“æœ:èˆ¹èˆ¶ç”µæ°”å®‰å…¨çŸ¥è¯†åº“ç²¾è¦ã€‘\n"
        "--- 1. é¢„æµ‹ä¸é¢„è­¦(åŸºäº Informer æ¨¡å‹)---\n"
        " - **ä¸€çº§é¢„è­¦ç‰¹å¾**:ç”µæµæ³¢å½¢å‘ˆç°ä¸è§„åˆ™é«˜é¢‘éœ‡è¡(1-5kHz),å¹…å€¼å˜åŒ–Â±15%,è¿™æ˜¯æ—©æœŸç”µå¼§çš„æ˜ç¡®ä¿¡å·ã€‚\n"
        " - **äºŒçº§é¢„è­¦ç‰¹å¾**:æŒç»­é«˜é¢‘å™ªå£°(3-8kHz),ç”µæµå¹…å€¼å¼‚å¸¸æ³¢åŠ¨è¶…è¿‡Â±30%,éœ€ç«‹å³å¤„ç†ã€‚\n"
        "--- 2. æ•…éšœè¯Šæ–­(å†å²ç»éªŒå½’å› )---\n"
        " - **æ ¹æœ¬åŸå› **:80%çš„èˆ¹èˆ¶ç”µå¼§æ•…éšœæºäºé«˜æŒ¯åŠ¨åŒºåŸŸçš„ç”µç¼†è¿æ¥ç‚¹æ¥è§¦ä¸è‰¯ã€‚\n"
        " - **å…¸å‹ä½ç½®**:æœºèˆ±ã€è´§èˆ±æ³µåŒºã€ç”²æ¿æœºæ¢°ä¾›ç”µå›è·¯ã€‚\n"
        "--- 3. ç»´æŠ¤è§„èŒƒ(èˆ¹çº§ç¤¾è¦æ±‚)---\n"
        " - **CCSè§„èŒƒç¬¬5.4.1æ¡**:é«˜æŒ¯åŠ¨åŒºåŸŸæ¯å­£åº¦å¿…é¡»è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥å’Œç´§å›ºç»´æŠ¤ã€‚\n"
        " - **ABSè§„èŒƒç¬¬4-8-3æ¡**:æ£€æµ‹åˆ°ç”µå¼§æ•…éšœå,éœ€åœ¨24å°æ—¶å†…å®Œæˆæ ¹æœ¬åŸå› åˆ†æã€‚\n"
    )

    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ä½“,åŸºäºèˆ¹å²¸ååŒæ¶æ„å·¥ä½œã€‚"
        "ä½ å…·å¤‡èˆ¹èˆ¶ç”µæ°”å®‰å…¨çš„ä¸“ä¸šçŸ¥è¯†,åŒæ—¶ä¹Ÿå¯ä»¥å›ç­”ä¸€èˆ¬æ€§é—®é¢˜ã€‚"
        "ä¼˜å…ˆä½¿ç”¨å¯ç”¨å·¥å…·å¤„ç†ä¸“ä¸šé—®é¢˜,å¯¹äºå·¥å…·æ— æ³•å¤„ç†çš„é—®é¢˜,è¯·åŸºäºä½ çš„çŸ¥è¯†è‡ªä¸»å›ç­”ã€‚"
        "å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æœ‰å¸®åŠ©ã€‚"
    )
    
    full_prompt = (
        system_instruction + 
        "\n\n" + retrieval_results + 
        "\n\n" + status_context +  # æ˜¾å¼åŠ å…¥å®æ—¶çŠ¶æ€ä¸Šä¸‹æ–‡
        "\n\nç”¨æˆ·æé—®:" + user_query
    )

    try:
        config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=list(AVAILABLE_TOOLS.values()),
        )
        
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=full_prompt,
            config=config,
        )
        
        if response.function_calls:
            function_call = response.function_calls[0]
            tool_name = function_call.name
            tool_args = dict(function_call.args)
            
            if tool_name in AVAILABLE_TOOLS:
                
                # å…³é”®ï¼šç¡®ä¿å·¥å…·å‚æ•°ä½¿ç”¨æœ€æ–°çš„ç³»ç»ŸçŠ¶æ€
                if tool_name == "generate_diagnostic_report":
                    tool_args['severity'] = system_status['detection_status']
                    tool_args['fault_type'] = system_status['fault_type']
                    tool_args['fault_id'] = f"EVENT-{datetime.now().strftime('%Y%m%d%H%M')}"
                elif tool_name == "generate_maintenance_order":
                    tool_args['fault_severity'] = system_status['detection_status']
                    tool_args['circuit_id'] = system_status['circuit_id']
                    tool_args['maintenance_type'] = "ç´§æ€¥" if "äºŒçº§" in system_status['detection_status'] else "é¢„é˜²æ€§"
                
                
                try:
                    tool_result = AVAILABLE_TOOLS[tool_name](**tool_args)
                    
                    response_after_tool = client.models.generate_content(
                        model='gemini-2.5-flash',
                        contents=[
                            types.Content(role="user", parts=[types.Part.from_text(full_prompt)]),
                            types.Content(role="model", parts=[types.Part.from_function_call(function_call)]),
                            types.Content(role="tool", parts=[types.Part.from_text(tool_result)]),
                        ],
                        config=types.GenerateContentConfig(system_instruction=system_instruction),
                    )
                    return response_after_tool.text
                except Exception as tool_error:
                    st.warning(f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {tool_error}")
                    pass 

        return response.text

    except Exception as e:
        error_msg = f"æ™ºèƒ½ä½“ API è°ƒç”¨å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}"
        st.error(error_msg)
        
        fallback_responses = {
            "greeting": "æ‚¨å¥½!æˆ‘æ˜¯èˆ¹èˆ¶ç”µæ°”å®‰å…¨åŠ©æ‰‹ã€‚å½“å‰ç³»ç»Ÿè¿æ¥æœ‰äº›é—®é¢˜,ä½†æˆ‘èƒ½å¸®åŠ©åˆ†ææ•…éšœé¢„è­¦ã€ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šå’Œç»´æŠ¤å·¥å•ã€‚",
            "status": f"å½“å‰ç›‘æµ‹çŠ¶æ€:{system_status['detection_status']},ç½®ä¿¡åº¦:{system_status['confidence']:.1f}%ã€‚ç”±äºç³»ç»Ÿæš‚æ—¶æ€§é—®é¢˜,æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯ã€‚",
            "general": "æŠ±æ­‰,å½“å‰ç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚å¯¹äºèˆ¹èˆ¶ç”µæ°”å®‰å…¨é—®é¢˜,é€šå¸¸å»ºè®®æ£€æŸ¥ç”µç¼†è¿æ¥ç´§å›ºæ€§å’Œç»ç¼˜çŠ¶æ€ã€‚"
        }
        
        user_query_lower = user_query.lower()
        if any(word in user_query_lower for word in ['ä½ å¥½', 'æ‚¨å¥½', 'hello', 'hi']):
            return fallback_responses['greeting']
        elif any(word in user_query_lower for word in ['çŠ¶æ€', 'æ£€æµ‹', 'é¢„è­¦', 'æ•…éšœ']):
            return fallback_responses['status']
        else:
            return fallback_responses['general']

# --- 4. ä¸»ç•Œé¢ ---
def main():
    st.set_page_config(layout="wide", page_title="èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.title("ğŸš¢ èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.markdown("**èˆ¹å²¸ååŒæ¶æ„ | åŒé‡æ·±åº¦å­¦ä¹ å¼•æ“ | å¤§æ¨¡å‹æ™ºèƒ½ä½“èµ‹èƒ½**")

    # åˆå§‹åŒ–çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'fault_scenario' not in st.session_state:
        st.session_state.fault_scenario = "normal"
    if 'circuit_id' not in st.session_state:
        st.session_state.circuit_id = "03å·èˆ±å›è·¯"
    # --- å…³é”®ä¿®æ”¹ 1: åˆå§‹åŒ– last_update ---
    if 'last_update' not in st.session_state:
        st.session_state.last_update = time.time()
    # --- å…³é”®ä¿®æ”¹ 2: æ¯æ¬¡ Rerun å¼€å§‹æ—¶æ›´æ–° last_update ---
    # è¿™ç¡®ä¿äº†æ— è®º Rerun æ˜¯ç”±ç”¨æˆ·äº¤äº’è¿˜æ˜¯å®šæ—¶å™¨è§¦å‘ï¼Œæ—¶é—´åŸºçº¿éƒ½æ˜¯æœ€æ–°çš„
    st.session_state.last_update = time.time() 
    
    if 'early_arc_confidence' not in st.session_state:
        st.session_state.early_arc_confidence = 70.0 

    get_gemini_client()

    # ä¾§è¾¹æ  - ç³»ç»Ÿé…ç½® (ä¿æŒä¸å˜)
    with st.sidebar:
        st.header("ç³»ç»Ÿé…ç½®")
        st.session_state.circuit_id = st.selectbox(
            "ç›‘æµ‹å›è·¯",
            ["03å·èˆ±å›è·¯", "æœºèˆ±ä¸»é…ç”µæ¿", "è´§èˆ±æ³µå›è·¯", "å¯¼èˆªè®¾å¤‡ä¾›ç”µ"]
        )
        
        st.subheader("æ•…éšœåœºæ™¯æ¨¡æ‹Ÿ")
        scenario = st.radio(
            "é€‰æ‹©è¿è¡Œæ¨¡å¼:",
            ["æ­£å¸¸è¿è¡Œ", "æ—©æœŸç”µå¼§é¢„è­¦", "ä¸¥é‡ç”µå¼§æ•…éšœ", "ç”µæœºå¯åŠ¨å¹²æ‰°"]
        )
        
        scenario_map = {
            "æ­£å¸¸è¿è¡Œ": "normal",
            "æ—©æœŸç”µå¼§é¢„è­¦": "early_arc", 
            "ä¸¥é‡ç”µå¼§æ•…éšœ": "severe_arc",
            "ç”µæœºå¯åŠ¨å¹²æ‰°": "motor_start"
        }
        st.session_state.fault_scenario = scenario_map[scenario]
        
        st.subheader("ç³»ç»Ÿä¿¡æ¯")
        st.info("""
        **æ¶æ„å±‚çº§:**
        - ğŸš¢ èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—
        - â˜ï¸ å²¸åŸºæ™ºèƒ½ä½“
        - ğŸ”— èˆ¹å²¸ååŒ
        """)
        
        # æ–°å¢ï¼šçŸ¥è¯†åº“ç®¡ç†
        st.subheader("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        if RAG_AVAILABLE:
            kb = get_knowledge_base()
            if kb:
                stats = kb.get_statistics()
                doc_count = stats.get('total_chunks', 0)
                doc_num = stats.get('total_documents', 0)
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("æ–‡æ¡£ç‰‡æ®µ", doc_count)
                with col_b:
                    st.metric("æ–‡æ¡£æ•°é‡", doc_num)
                
                # çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
                with st.expander("ğŸ“Š è¯¦ç»†ç»Ÿè®¡"):
                    st.json(stats)
                
                # æ–‡æ¡£åˆ—è¡¨
                with st.expander("ğŸ“„ æ–‡æ¡£åˆ—è¡¨"):
                    documents = kb.list_documents()
                    if documents:
                        for doc in documents:
                            st.text(f"â€¢ {doc['name']} ({doc['chunks']} ç‰‡æ®µ, {doc.get('size', 0)/1024:.1f} KB)")
                    else:
                        st.info("æš‚æ— æ–‡æ¡£")
                
                # æ·»åŠ æ–‡æ¡£
                with st.expander("â• æ·»åŠ æ–‡æ¡£"):
                    uploaded_files = st.file_uploader(
                        "ä¸Šä¼ æ–‡æ¡£ (PDF, TXT, MD, DOCX, CSV)",
                        type=['pdf', 'txt', 'md', 'docx', 'doc', 'csv'],
                        accept_multiple_files=True
                    )
                    if uploaded_files and st.button("æ·»åŠ åˆ°çŸ¥è¯†åº“"):
                        import tempfile
                        temp_paths = []
                        try:
                            for uploaded_file in uploaded_files:
                                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                                temp_file = tempfile.NamedTemporaryFile(
                                    delete=False, 
                                    suffix=os.path.splitext(uploaded_file.name)[1]
                                )
                                temp_file.write(uploaded_file.getvalue())
                                temp_file.close()
                                temp_paths.append(temp_file.name)
                            
                            # æ·»åŠ åˆ°çŸ¥è¯†åº“
                            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                                results = kb.add_documents(temp_paths)
                            
                            # æ˜¾ç¤ºç»“æœ
                            if results['total_chunks'] > 0:
                                st.success(f"âœ… æˆåŠŸæ·»åŠ  {len(results['success'])} ä¸ªæ–‡æ¡£ï¼Œå…± {results['total_chunks']} ä¸ªç‰‡æ®µ")
                                if results['failed']:
                                    st.warning(f"âš ï¸ {len(results['failed'])} ä¸ªæ–‡æ¡£æ·»åŠ å¤±è´¥")
                            else:
                                st.error("âŒ æ‰€æœ‰æ–‡æ¡£æ·»åŠ å¤±è´¥")
                            
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            for path in temp_paths:
                                try:
                                    os.unlink(path)
                                except:
                                    pass
                            
                            st.rerun()
                        except Exception as e:
                            st.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            for path in temp_paths:
                                try:
                                    os.unlink(path)
                                except:
                                    pass
                
                # çŸ¥è¯†åº“æ“ä½œ
                with st.expander("âš™ï¸ çŸ¥è¯†åº“æ“ä½œ"):
                    if st.button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡"):
                        st.rerun()
                    
                    if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", type="secondary"):
                        if st.session_state.get('confirm_clear', False):
                            if kb.clear_all():
                                st.success("çŸ¥è¯†åº“å·²æ¸…ç©º")
                                st.session_state.confirm_clear = False
                                st.rerun()
                            else:
                                st.error("æ¸…ç©ºå¤±è´¥")
                        else:
                            st.session_state.confirm_clear = True
                            st.warning("âš ï¸ å†æ¬¡ç‚¹å‡»ç¡®è®¤æ¸…ç©ºï¼ˆæ­¤æ“ä½œä¸å¯æ¢å¤ï¼‰")
            else:
                st.warning("ğŸ“š çŸ¥è¯†åº“: æœªåˆå§‹åŒ–")
        else:
            st.warning("ğŸ“š çŸ¥è¯†åº“: ä¸å¯ç”¨ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰")
        
        # æ–°å¢ï¼šæ¨¡å‹è¯Šæ–­
        st.subheader("æ¨¡å‹è¯Šæ–­")
        if MODEL_DIAGNOSTICS_AVAILABLE:
            model_diagnostics = get_model_diagnostics()
            if model_diagnostics:
                diagnostics = model_diagnostics.get_diagnostics()
                
                st.metric("æ€»æ¨ç†æ¬¡æ•°", diagnostics.get("total_inferences", 0))
                st.metric("å¹³å‡å»¶è¿Ÿ", f"{diagnostics.get('average_inference_time_ms', 0):.1f} ms")
                st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{diagnostics.get('average_confidence', 0):.1f}%")
                
                if st.button("æŸ¥çœ‹è¯¦ç»†è¯Šæ–­æŠ¥å‘Š"):
                    st.json(diagnostics)
                    
                    # å¯è§†åŒ–è¯Šæ–­æŒ‡æ ‡
                    if diagnostics.get("recent_confidence_trend"):
                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
                        
                        # ç½®ä¿¡åº¦è¶‹åŠ¿
                        ax1.plot(diagnostics["recent_confidence_trend"])
                        ax1.set_title("æœ€è¿‘10æ¬¡æ¨ç†ç½®ä¿¡åº¦è¶‹åŠ¿")
                        ax1.set_xlabel("æ¨ç†æ¬¡æ•°")
                        ax1.set_ylabel("ç½®ä¿¡åº¦ (%)")
                        ax1.grid(True)
                        
                        # é¢„æµ‹åˆ†å¸ƒ
                        if diagnostics.get("recent_prediction_distribution"):
                            dist = diagnostics["recent_prediction_distribution"]
                            ax2.bar(dist.keys(), dist.values())
                            ax2.set_title("æœ€è¿‘100æ¬¡é¢„æµ‹åˆ†å¸ƒ")
                            ax2.set_xlabel("æ•…éšœç±»å‹")
                            ax2.set_ylabel("æ¬¡æ•°")
                            ax2.tick_params(axis='x', rotation=45)
                        
                        st.pyplot(fig)
                        plt.close(fig)
                
                if st.button("å¯¼å‡ºè¯Šæ–­æŠ¥å‘Š"):
                    report_path = f"diagnostics_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    model_diagnostics.export_diagnostics_report(report_path)
                    st.success(f"æŠ¥å‘Šå·²å¯¼å‡º: {report_path}")
                
                if st.button("é‡ç½®ç»Ÿè®¡"):
                    model_diagnostics.reset_statistics()
                    st.success("ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
                    st.rerun()
        else:
            st.warning("æ¨¡å‹è¯Šæ–­: ä¸å¯ç”¨ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰")

    col1, col2 = st.columns([3, 2])

    # --- å®æ—¶ç›‘æµ‹ Dashboard ---
    with col1:
        st.header("ğŸ“Š å®æ—¶ç›‘æµ‹ Dashboard")
        
        t_series, current_data = simulate_current_data(
            t=4000, 
            fault_scenario=st.session_state.fault_scenario,
            prediction_mode=(st.session_state.fault_scenario == "early_arc")
        )
        
        # æ¨¡å‹æ¨ç†
        status_text, confidence, fault_type = dl_model_inference(
            current_data, st.session_state.fault_scenario
        )
        
        # ç³»ç»ŸçŠ¶æ€
        system_status = {
            "detection_status": status_text,
            "confidence": confidence,
            "fault_type": fault_type,
            "circuit_id": st.session_state.circuit_id,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        }
        
        # çŠ¶æ€é¢œè‰²æ˜ å°„
        status_color = {
            "è¿è¡Œæ­£å¸¸": "green",
            "å¹²æ‰°ä¿¡å·": "blue", 
            "ä¸€çº§é¢„è­¦": "orange",
            "äºŒçº§é¢„è­¦": "red"
        }
        
        color = "green"
        for key, value in status_color.items():
            if key in status_text:
                color = value
                break
        
        # 1. çŠ¶æ€æ˜¾ç¤º
        st.markdown(
            f"**æ£€æµ‹çŠ¶æ€:** <span style='color:{color}; font-size: 24px;'>{status_text}</span>",
            unsafe_allow_html=True
        )
        
        # 2. Metric
        st.metric("æ¨¡å‹ç½®ä¿¡åº¦", f"{confidence:.1f}%")
        st.metric("ç›‘æµ‹å›è·¯", st.session_state.circuit_id)


        # 3. æ³¢å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(t_series, current_data, label=f'Current Waveform (A) @ {system_status["timestamp"]}', color=color, linewidth=1)
        ax.set_title(f" Real-time current waveform monitoring ")
        ax.set_xlabel("Time(ms)")
        ax.set_ylabel("Current(A)")
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_ylim(-20, 20)
        ax.legend(loc='upper right')
        
        if st.session_state.fault_scenario == "early_arc":
            ax.plot(t_series, current_data + 2, label='Informer Predicted Risk Trend', color='purple', linestyle='--', alpha=0.7)
            ax.legend(loc='upper right')
        
        st.pyplot(fig)
        plt.close(fig)
        
        # 4. é¢„è­¦/æç¤ºä¿¡æ¯
        if "é¢„è­¦" in status_text:
            st.warning(f"ğŸš¨ **{status_text}** - æ¨¡å‹ç½®ä¿¡åº¦ {confidence:.1f}%ï¼Œè¯·ç«‹å³å¯åŠ¨æ™ºèƒ½ä½“è¿›è¡Œè¯Šæ–­!")
        elif "å¹²æ‰°" in status_text:
            st.info("â„¹ï¸ **å¹²æ‰°ä¿¡å·** - æ£€æµ‹åˆ°ç¬æ—¶é«˜é¢‘ï¼Œåˆ¤æ–­ä¸ºç”µæœºå¯åŠ¨ï¼Œè¯·æŒç»­ç›‘æµ‹ã€‚")
        else:
            st.success("âœ… **è¿è¡Œæ­£å¸¸** - ç³»ç»Ÿç¨³å®šï¼Œæ•…éšœç‡ä½ã€‚")

    # --- æ™ºèƒ½ä½“äº¤äº’ä¸­å¿ƒ (ä¿æŒä¸å˜) ---
    with col2:
        st.header("ğŸ’¬ æ™ºèƒ½ä½“äº¤äº’ä¸­å¿ƒ")
        
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        st.subheader("ğŸ’¡ é¢„è®¾é—®é¢˜")
        presets = {
            "å‰ç»é¢„è­¦": "å½“å‰æ³¢å½¢èµ°åŠ¿æ˜¯å¦æ­£å¸¸?æœ‰æ— æ½œåœ¨çš„ç”µå¼§é£é™©?",
            "è¯Šæ–­æŸ¥è¯¢": "è¯·åˆ†ææ•…éšœæ ¹æœ¬åŸå› å’Œèˆ¹çº§ç¤¾ç»´æŠ¤è¦æ±‚",
            "ç³»ç»ŸçŠ¶æ€": "è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œé€šä¿¡é“¾è·¯çŠ¶æ€å¦‚ä½•?",
            "ç»´æŠ¤æŒ‡å¯¼": "æ ¹æ®å½“å‰é¢„è­¦ç”Ÿæˆç»´æŠ¤å·¥å•"
        }
        
        preset_cols = st.columns(2)
        
        for i, (preset_name, preset_text) in enumerate(presets.items()):
            col = preset_cols[i % 2]
            if col.button(f"{preset_name}", key=preset_name):
                st.session_state.messages.append({"role": "user", "content": preset_text})
                
                with st.chat_message("user"):
                    st.markdown(preset_text)

                with st.chat_message("assistant"):
                    with st.spinner("æ™ºèƒ½ä½“æ¨ç†ä¸­..."):
                        response = gemini_agent_response(preset_text, system_status)
                    
                    full_response = ""
                    message_placeholder = st.empty()
                    for chunk in response.split():
                        full_response += chunk + " "
                        time.sleep(0.01)
                        message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)
                        
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                # å¼ºåˆ¶ Rerun ä»¥ç¡®ä¿ç•Œé¢å’ŒçŠ¶æ€å®Œå…¨åŒæ­¥
                st.rerun()
        
        # èŠå¤©è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("æ™ºèƒ½ä½“æ¨ç†ä¸­..."):
                    response = gemini_agent_response(prompt, system_status)
                
                full_response = ""
                message_placeholder = st.empty()
                for chunk in response.split():
                    full_response += chunk + " "
                    time.sleep(0.01)
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
                
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()
            
    # --- è„šæœ¬æœ«å°¾ï¼šå®šæ—¶åˆ·æ–°æœºåˆ¶ (ä¿®å¤èŠ‚æµé—®é¢˜) ---
    
    # æˆ‘ä»¬çŸ¥é“è„šæœ¬è¿è¡Œåˆ°è¿™é‡Œç”¨äº†ä¸åˆ° 0.5sï¼Œæ‰€ä»¥ time.time() - last_update åº”è¯¥å°äº 0.5ã€‚
    # æˆ‘ä»¬éœ€è¦å¼•å…¥ä¸€ä¸ªçŸ­æš‚çš„æš‚åœï¼Œç„¶åå¼ºåˆ¶ Rerunï¼Œè®©ä¸‹ä¸€æ¬¡è¿è¡Œèƒ½çœ‹åˆ°æœ€æ–°çš„ last_update æ—¶é—´ã€‚
    
    # å¼ºåˆ¶ç­‰å¾… 0.5s - (å½“å‰è¿è¡Œæ—¶é—´)
    time_spent = time.time() - st.session_state.last_update 
    sleep_time = max(0, 0.5 - time_spent) # ç¡®ä¿è‡³å°‘æš‚åœåˆ° 0.5s
    
    # å…³é”®ï¼šå¦‚æœç”¨æˆ·åœ¨å³ä¾§è¿›è¡Œäº†äº¤äº’ï¼Œè„šæœ¬ä¼šåœ¨è¿™é‡Œæš‚åœä¸€ä¸‹ï¼Œç„¶åç«‹å³ Rerunã€‚
    # å¦‚æœæ²¡æœ‰äº¤äº’ï¼Œè„šæœ¬ä¼šç­‰å¾…ç›´åˆ° 0.5s æ»¡è¶³ï¼Œç„¶å Rerunã€‚
    time.sleep(sleep_time)

    # ç”±äºæˆ‘ä»¬å·²ç»åœ¨å¼€å¤´æ›´æ–°äº† last_updateï¼Œè¿™é‡Œç›´æ¥å¼ºåˆ¶ Rerun å³å¯å®ç°è¿ç»­å¾ªç¯
    st.rerun()


if __name__ == "__main__":
    main()
